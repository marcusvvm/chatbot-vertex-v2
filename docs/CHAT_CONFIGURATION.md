# Chat Configuration Guide

> Documenta√ß√£o t√©cnica sobre as configura√ß√µes do chat e tuning do modelo Gemini.

---

## üìã Vis√£o Geral

Este documento descreve as configura√ß√µes dispon√≠veis para o sistema de chat RAG, incluindo par√¢metros do modelo, estrat√©gias de diagn√≥stico e recomenda√ß√µes de tuning.

**Arquivo de Configura√ß√£o:** `app/core/chat_config.py`

---

## üîß Par√¢metros Configur√°veis

### Modelo

```python
MODEL_NAME = "gemini-2.5-pro"  # Atual (com THINKING_BUDGET controlado)
# MODEL_NAME = "gemini-1.5-pro"  # Alternativo (sem thinking tokens)
```

| Modelo           | Thinking Tokens | THINKING_BUDGET | Recomenda√ß√£o                            |
| ---------------- | --------------- | --------------- | --------------------------------------- |
| `gemini-2.5-pro` | ‚úÖ Usa           | Configur√°vel    | ‚úÖ Recomendado (com budget limitado)     |
| `gemini-1.5-pro` | ‚ùå N√£o usa       | N/A             | Alternativa para respostas muito longas |

---

### Thinking Budget (Controle de Racioc√≠nio)

O `gemini-2.5-pro` usa tokens internos para "pensar" antes de responder. Sem controle, ele pode consumir a maioria do `max_output_tokens` em racioc√≠nio, deixando pouco espa√ßo para a resposta.

```python
# Thinking Budget Configuration (for gemini-2.5-pro)
THINKING_BUDGET = 1024  # Tokens reservados para racioc√≠nio interno
```

| Valor  | Comportamento             | Caso de Uso                                |
| ------ | ------------------------- | ------------------------------------------ |
| `128`  | M√≠nimo, respostas r√°pidas | Perguntas simples, baixa lat√™ncia          |
| `1024` | **Balanceado (atual)**    | Uso geral, equil√≠brio qualidade/velocidade |
| `2048` | Mais reasoning            | Tarefas complexas, multi-step              |
| `-1`   | Din√¢mico                  | Modelo decide automaticamente              |

**Implementa√ß√£o em `vertex_service.py`:**
```python
thinking_config=types.ThinkingConfig(
    thinking_budget=ChatConfig.THINKING_BUDGET
)
```

---

### Gera√ß√£o de Conte√∫do

```python
GENERATION_CONFIG = {
    "temperature": 0.2,        # Respostas mais determin√≠sticas
    "top_p": 0.8,              # Nucleus sampling
    "top_k": 40,               # Limite de tokens considerados
    "max_output_tokens": 16384 # Limite de tokens na resposta
}
```

| Par√¢metro           | Valor | Descri√ß√£o                                  |
| ------------------- | ----- | ------------------------------------------ |
| `temperature`       | 0.2   | Baixa = respostas mais consistentes        |
| `top_p`             | 0.8   | Probabilidade acumulada para sampling      |
| `top_k`             | 40    | Top K tokens a considerar                  |
| `max_output_tokens` | 16384 | M√°ximo de tokens gerados (inclui thinking) |

---

### RAG e Timeout

```python
RAG_RETRIEVAL_TOP_K = 5    # Chunks retornados na busca
TIMEOUT_SECONDS = 90.0     # Timeout da requisi√ß√£o
MAX_HISTORY_LENGTH = 20    # Mensagens no hist√≥rico
```

---

## üîç Diagn√≥stico de Truncamento

### Problema Identificado

Respostas estavam sendo cortadas no meio, sem completar a informa√ß√£o solicitada.

### Estrat√©gia de Logging

Adicionado logging tempor√°rio em `app/services/vertex_service.py` para capturar metadados da resposta do Vertex AI:

```python
# === DIAGNOSTIC LOGGING (TEMPORARY) ===
import logging
logger = logging.getLogger(__name__)

if hasattr(response, 'candidates') and response.candidates:
    candidate = response.candidates[0]
    finish_reason = getattr(candidate, 'finish_reason', 'UNKNOWN')
    logger.warning(f"[DIAG] finish_reason: {finish_reason}")
    
    if hasattr(candidate, 'safety_ratings'):
        logger.warning(f"[DIAG] safety_ratings: {candidate.safety_ratings}")
    
    if hasattr(response, 'usage_metadata'):
        logger.warning(f"[DIAG] usage_metadata: {response.usage_metadata}")

response_text = response.text
logger.warning(f"[DIAG] response_length: {len(response_text)} chars")
# === END DIAGNOSTIC LOGGING ===
```

### Resultados Obtidos

#### Teste 1: `max_output_tokens = 2048` com `gemini-2.5-pro`

```
finish_reason: FinishReason.MAX_TOKENS
thoughts_token_count: 1457
candidates_token_count: 589
total_token_count: 3139
response_length: 1776 chars
```

**An√°lise:** Modelo usou 1457 tokens para "pensar" + 589 para resposta = 2046 (limite atingido).

#### Teste 2: `max_output_tokens = 8192` com `gemini-2.5-pro`

```
finish_reason: FinishReason.MAX_TOKENS
thoughts_token_count: 7863
candidates_token_count: 327
total_token_count: 9283
response_length: 737 chars
```

**An√°lise:** Modelo usou 7863 tokens para "pensar" (96%!), deixando apenas 327 para resposta.

---

## üéØ Conclus√µes e Recomenda√ß√µes

### Por que as respostas eram truncadas?

O modelo `gemini-2.5-pro` possui um recurso de "thinking tokens" que consome a maior parte do `max_output_tokens` para racioc√≠nio interno, deixando pouco espa√ßo para a resposta vis√≠vel.

### Configura√ß√£o Atual (Recomendada)

| Par√¢metro             | Valor            | Justificativa                                |
| --------------------- | ---------------- | -------------------------------------------- |
| `MODEL_NAME`          | `gemini-2.5-pro` | Modelo mais avan√ßado com thinking controlado |
| `THINKING_BUDGET`     | 1024             | Balanceado: qualidade + velocidade           |
| `max_output_tokens`   | 16384            | Margem ampla para respostas longas           |
| `RAG_RETRIEVAL_TOP_K` | 5                | Contexto suficiente sem overhead             |
| `TIMEOUT_SECONDS`     | 90.0             | Acomodar respostas complexas                 |

### Quando Ajustar THINKING_BUDGET

| Cen√°rio                  | A√ß√£o                                                            |
| ------------------------ | --------------------------------------------------------------- |
| Respostas truncadas      | Verificar se `THINKING_BUDGET` + resposta > `max_output_tokens` |
| Respostas superficiais   | Aumentar para 2048                                              |
| Lat√™ncia alta            | Reduzir para 128                                                |
| Perguntas simples lentas | Reduzir para 128                                                |

---

## üõ†Ô∏è Manuten√ß√£o

### Logging de Diagn√≥stico (Removido)

O logging tempor√°rio foi removido ap√≥s diagn√≥stico. Para reativar, adicione em `vertex_service.py` ap√≥s `response = ...`:

```python
import logging
logger = logging.getLogger(__name__)
logger.warning(f"[DIAG] finish_reason: {response.candidates[0].finish_reason}")
logger.warning(f"[DIAG] usage_metadata: {response.usage_metadata}")
```

### Monitorar Finish Reason

Se problemas de truncamento retornarem, verifique:
1. `finish_reason` nos logs.
2. Se `finish_reason: MAX_TOKENS`, aumente `max_output_tokens`.
3. Se `finish_reason: SAFETY`, revise `safety_settings`.

---

**√öltima Atualiza√ß√£o:** Dezembro 2025
